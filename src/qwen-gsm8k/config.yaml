# GRPO + LoRA Training Configuration for Qwen2.5-3B on GSM8K
# Target: Lambda Labs gpu_1x_a100_sxm4 (40GB VRAM)

# Algorithm Configuration
algorithm:
  adv_estimator: grpo  # Group Relative Policy Optimization

# Data Configuration
data:
  train_files: ~/data/gsm8k/train.parquet
  val_files: ~/data/gsm8k/test.parquet
  train_batch_size: 16  # Reduced for 40GB GPU
  val_batch_size: 16
  max_prompt_length: 512
  max_response_length: 1024

# Model Configuration
actor_rollout_ref:
  model:
    path: Qwen/Qwen2.5-3B-Instruct
    # LoRA Configuration
    use_lora: true
    lora_rank: 64
    lora_alpha: 32
    lora_dropout: 0.05
    lora_target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj

  # Actor (Policy) Configuration
  actor:
    use_kl_loss: true
    kl_loss_coef: 0.001
    learning_rate: 1.0e-6

  # Rollout (Generation) Configuration
  rollout:
    name: vllm
    gpu_memory_utilization: 0.6
    tensor_parallel_size: 1
    rollout_count: 5  # Number of samples per prompt

# Trainer Configuration
trainer:
  total_epochs: 15
  save_freq: 5  # Save checkpoint every 5 epochs
  project_name: qwen-gsm8k-grpo
  experiment_name: qwen2.5-3b-lora
  logger:
    - wandb  # Optional: set WANDB_API_KEY to enable

# Output Configuration
output:
  checkpoint_dir: ~/checkpoints
  logging_dir: ~/logs

# Reward Configuration
reward:
  reward_fn: reward.compute_score  # Custom reward function
